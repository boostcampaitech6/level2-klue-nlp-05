{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pickle\n",
    "import ast\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from omegaconf import OmegaConf\n",
    "from dataset_utils import load_data, label_to_num, tokenized_dataset\n",
    "from datasets import RE_Dataset\n",
    "from metrics import compute_metrics\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.new_tokens = ['[PER]', '[ORG]', '[DAT]', '[LOC]', '[POH]', '[NOH]']\n",
    "        self.tokenizer.add_tokens(self.new_tokens)\n",
    "        self.LABEL_TO_ID = {'no_relation': 0, 'org:top_members/employees': 1, 'org:members': 2, 'org:product': 3, 'per:title': 4, 'org:alternate_names': 5, 'per:employee_of': 6, \\\n",
    "                'org:place_of_headquarters': 7, 'per:product': 8, 'org:number_of_employees/members': 9, 'per:children': 10, 'per:place_of_residence': 11, 'per:alternate_names': 12, \\\n",
    "                'per:other_family': 13, 'per:colleagues': 14, 'per:origin': 15, 'per:siblings': 16, 'per:spouse': 17, 'org:founded': 18, 'org:political/religious_affiliation': 19, \\\n",
    "                'org:member_of': 20, 'per:parents': 21, 'org:dissolved': 22, 'per:schools_attended': 23, 'per:date_of_death': 24, 'per:date_of_birth': 25, 'per:place_of_birth': 26, \\\n",
    "                'per:place_of_death': 27, 'org:founded_by': 28, 'per:religion': 29}\n",
    "        \n",
    "    def word_idx_extract(self, words, ns, ne):\n",
    "        \n",
    "        word_indices = []\n",
    "        start_index = 0\n",
    "\n",
    "        for word in words:\n",
    "            end_index = start_index + len(word) - 1\n",
    "            word_indices.append((start_index, end_index))\n",
    "            start_index = end_index + 2\n",
    "\n",
    "        word_idx=[]\n",
    "        for i, (start, end) in enumerate(word_indices):\n",
    "            if ns in range(start, end + 1) or ne in range(start, end + 1):\n",
    "                word_idx.append(i)\n",
    "                \n",
    "        return word_idx[0] , word_idx[-1]\n",
    "\n",
    "\n",
    "    def token_location(self, list1, list2):\n",
    "        for i in range(len(list1) - len(list2) + 1):\n",
    "            if list1[i:i + len(list2)] == list2:\n",
    "                index = i\n",
    "                return i, i+len(list2)-1\n",
    "\n",
    "    def tokenize(self, sentence, subject_word, object_word, subj_type, obj_type, ss, se, os, oe):\n",
    "        print(sentence,subject_word, object_word, subj_type, obj_type, ss, se, os, oe)\n",
    "        words = sentence.split()\n",
    "\n",
    "        sws, swe = self.word_idx_extract(words, ss,se)\n",
    "        ows, owe = self.word_idx_extract(words, os,oe)\n",
    "\n",
    "        subj_tokens= self.tokenizer.tokenize(subject_word)\n",
    "        obj_tokens= self.tokenizer.tokenize(object_word)\n",
    "\n",
    "        sents =[]\n",
    "        subj_type , obj_type = f\"[{subj_type}]\", f\"[{obj_type}]\"\n",
    "        subj_token_collect = []\n",
    "        obj_token_collect = []\n",
    "\n",
    "        for idx, word in enumerate(words):\n",
    "            tokens = self.tokenizer.tokenize(word)\n",
    "            if idx not in range(sws,swe+1) and idx not in range(ows,owe+1):\n",
    "                sents.extend(tokens)\n",
    "\n",
    "            else:\n",
    "                if sws <= idx and idx <= swe:\n",
    "                    subj_token_collect.extend(tokens)\n",
    "                    if idx == swe:\n",
    "                        ts, te = self.token_location(subj_token_collect, subj_tokens)\n",
    "                        tokens = subj_token_collect[:ts] + ['@'] + ['*'] + [subj_type] + ['*'] + subj_tokens + ['@'] + subj_token_collect[te+1:]\n",
    "                        new_ss = len(sents) + len(subj_token_collect[:ts])\n",
    "                        sents.extend(tokens)\n",
    "\n",
    "                if ows <= idx and idx <= owe:\n",
    "                    obj_token_collect.extend(tokens)\n",
    "                    if idx == owe:\n",
    "                        ts, te = self.token_location(obj_token_collect, obj_tokens)\n",
    "                        tokens = obj_token_collect[:ts] + [\"#\"] + ['^'] + [obj_type] + ['^'] + obj_tokens + [\"#\"] + obj_token_collect[te+1:]\n",
    "                        new_os = len(sents) + len(obj_token_collect[:ts])\n",
    "                        sents.extend(tokens)\n",
    "\n",
    "        sents = sents[:self.args.model.max_seq_length - 2]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(sents)\n",
    "        input_ids = self.tokenizer.build_inputs_with_special_tokens(input_ids)\n",
    "        print(sents)\n",
    "        return input_ids, new_ss + 1, new_os + 1\n",
    "\n",
    "    def read(self, file_in):\n",
    "        features = []\n",
    "        with open(file_in, \"r\") as fh:\n",
    "            data = pd.read_csv(fh)\n",
    "\n",
    "        for _, d in tqdm(data.iterrows()):\n",
    "            ss, se = int(d['subject_start_idx']), int(d['subject_end_idx'])\n",
    "            os, oe = int(d['object_start_idx']), int(d['object_end_idx'])\n",
    "            input_ids, new_ss, new_os = self.tokenize(d['sentence'],d['subject_word'],d['object_word'], d['subject_type'], d['object_type'], ss, se, os, oe)\n",
    "            rel = self.LABEL_TO_ID[d['label']]\n",
    "\n",
    "            feature = {\n",
    "                'input_ids': input_ids,\n",
    "                'labels': rel,\n",
    "                'ss': new_ss,\n",
    "                'os': new_os,\n",
    "            }\n",
    "            features.append(feature)\n",
    "\n",
    "        return features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 245.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey Road》에 담은 노래다. 비틀즈 조지 해리슨 ORG PER 24 26 13 18\n",
      "['〈', 'So', '##me', '##th', '##ing', '〉', '는', '#', '^', '[PER]', '^', '조지', '해리', '##슨', '#', '##이', '쓰', '##고', '@', '*', '[ORG]', '*', '비틀즈', '@', '##가', '1969', '##년', '앨범', '《', 'Ab', '##be', '##y', 'Ro', '##ad', '》', '에', '담', '##은', '노래', '##다', '.']\n",
      "호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으로 재탄생한다. 민주평화당 대안신당 ORG ORG 19 23 14 17\n",
      "['호남', '##이', '기반', '##인', '바른', '##미', '##래', '##당', '·', '대안', '##신', '##당', '·', '@', '*', '[ORG]', '*', '민주', '##평', '##화', '##당', '@', '##이', '바른', '##미', '##래', '##당', '·', '#', '^', '[ORG]', '^', '대안', '##신', '##당', '#', '·', '@', '*', '[ORG]', '*', '민주', '##평', '##화', '##당', '@', '##이', '우여곡절', '끝', '##에', '합당', '##해', '민생', '##당', '(', '가칭', ')', '으로', '재', '##탄', '##생', '##한다', '.']\n",
      "K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터 관중 유치 성과와 마케팅 성과를 인정받아 ‘풀 스타디움상’과 ‘플러스 스타디움상’을 수상했다. 광주FC 한국프로축구연맹 ORG ORG 21 24 34 41\n",
      "['K', '##리그', '##2', '##에서', '성적', '1', '##위', '##를', '달리', '##고', '있', '##는', '@', '*', '[ORG]', '*', '광주', '##FC', '@', '##는', '지난', '26', '##일', '#', '^', '[ORG]', '^', '한국', '##프로', '##축구연맹', '#', '##으로', '##부터', '관중', '유치', '성과', '##와', '마케팅', '성과', '##를', '인정받', '##아', '‘', '풀', '스타디움', '##상', '’', '과', '‘', '플러스', '스타디움', '##상', '’', '을', '수상', '##했', '##다', '.']\n",
      "균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪고 있는 대구광역시에 행복박스를 전달했다고 10일 밝혔다. 아성다이소 박정부 ORG PER 13 17 22 24\n",
      "['균일', '##가', '생활', '##용품', '##점', '(', '주', ')', '@', '*', '[ORG]', '*', '아성', '##다이', '##소', '@', '(', '대표', '#', '^', '[PER]', '^', '박정', '##부', '#', ')', '는', '코', '##로나', '##19', '바이러스', '##로', '어려움', '##을', '겪', '##고', '있', '##는', '대구', '##광역시', '##에', '행복', '##박스', '##를', '전달', '##했', '##다고', '10', '##일', '밝혔', '##다', '.']\n",
      "1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8번으로 배정되었다. 요미우리 자이언츠 1967 ORG DAT 22 30 0 3\n",
      "['#', '^', '[DAT]', '^', '1967', '#', '##년', '프로', '야구', '드래프트', '1', '##순위', '##로', '@', '*', '[ORG]', '*', '요미우리', '자이언츠', '@', '##에', '##게', '입단', '##하면', '##서', '등', '##번', '##호', '##는', '8', '##번', '##으로', '배정', '##되', '##었', '##다', '.']\n",
      ": 유엔, 유럽 의회, 북대서양 조약 기구 (NATO), 국제이주기구, 세계 보건 기구 (WHO), 지중해 연합, 이슬람 협력 기구, 유럽 안보 협력 기구, 국제 통화 기금, 세계무역기구 그리고 프랑코포니. 북대서양 조약 기구 NATO ORG ORG 13 22 25 28\n",
      "[':', '유엔', ',', '유럽', '의회', ',', '@', '*', '[ORG]', '*', '북대', '##서', '##양', '조약', '기구', '@', '(', '#', '^', '[ORG]', '^', 'NA', '##TO', '#', ')', ',', '국제', '##이', '##주', '##기구', ',', '세계', '보건', '기구', '(', 'WHO', ')', ',', '지중해', '연합', ',', '이슬람', '협력', '기구', ',', '유럽', '안보', '협력', '기구', ',', '국제', '통화', '기금', ',', '세계무역', '##기구', '그리고', '프랑', '##코', '##포', '##니', '.']\n",
      "그에 따라 나폴리와 계약을 연장한 마라도나는 1989년 팀을 UEFA컵 정상으로 인도했으며 이듬해에는 유럽 챔피언 AC 밀란을 상대로 승리를 거두고 다시 한 번 세리에A에서 정상에 등극했다. AC 밀란 1989 ORG DAT 64 68 25 28\n",
      "['그', '##에', '따라', '나폴리', '##와', '계약', '##을', '연장', '##한', '마라', '##도', '##나', '##는', '#', '^', '[DAT]', '^', '1989', '#', '##년', '팀', '##을', 'UEFA', '##컵', '정상', '##으로', '인도', '##했', '##으며', '이듬해', '##에', '##는', '유럽', '챔피언', '@', '*', '[ORG]', '*', 'AC', '밀란', '@', '##을', '상대', '##로', '승리', '##를', '거두', '##고', '다시', '한', '번', '세리', '##에', '##A', '##에서', '정상', '##에', '등극', '##했', '##다', '.']\n",
      "박용오(朴容旿, 1937년 4월 29일(음력 3월 19일)(음력 3월 19일) ~ 2009년 11월 4일)는 서울에서 태어난 대한민국의 기업인으로 두산그룹 회장, KBO 총재 등을 역임했다. 박용오 1937년 4월 29일 PER DAT 0 2 9 20\n",
      "['@', '*', '[PER]', '*', '박용', '##오', '@', '(', '朴', '[UNK]', '[UNK]', ',', '#', '^', '[DAT]', '^', '1937', '##년', '4', '##월', '29', '##일', '#', '(', '음력', '3', '##월', '19', '##일', ')', '(', '음력', '3', '##월', '19', '##일', ')', '~', '2009', '##년', '11', '##월', '4', '##일', ')', '는', '서울', '##에서', '태어난', '대한민국', '##의', '기업인', '##으로', '두산', '##그룹', '회장', ',', 'KBO', '총재', '등', '##을', '역임', '##했', '##다', '.']\n",
      "중공군에게 온전히 대항할 수 없을 정도로 약해진 국민당은 타이베이로 수도를 옮기는 것을 결정해, 남아있는 중화민국군의 병력이나 국가, 개인의 재산등을 속속 타이완으로 옮기기 시작해, 12월에는 중앙 정부 기구도 모두 이전해 타이베이 시를 중화민국의 새로운 수도로 삼았다. 중화민국 타이베이 ORG LOC 59 62 32 35\n",
      "['중공', '##군', '##에', '##게', '온전히', '대항', '##할', '수', '없', '##을', '정도', '##로', '약해', '##진', '국민당', '##은', '#', '^', '[LOC]', '^', '타이', '##베이', '#', '##로', '수도', '##를', '옮기', '##는', '것', '##을', '결정', '##해', ',', '남아', '##있', '##는', '@', '*', '[ORG]', '*', '중화', '##민국', '@', '##군', '##의', '병력', '##이나', '국가', ',', '개인', '##의', '재산', '##등', '##을', '속속', '타이완', '##으로', '옮기', '##기', '시작', '##해', ',', '12', '##월', '##에', '##는', '중앙', '정부', '기구', '##도', '모두', '이전', '##해', '타이', '##베이', '시', '##를', '중화', '##민국', '##의', '새로운', '수도', '##로', '삼', '##았', '##다', '.']\n",
      "특히 김동연 전 경제부총리를 비롯한 김두관 국회의원, 안규백 국회의원, 김종민 국회의원, 오제세 국회의원, 최운열 국회의원, 김정우 국회의원, 권칠승 국회의원, 맹성규 국회의원등 더불어민주당 국회의원 8명이 영상 축하 메세지를 보내 눈길을 끌었다. 안규백 더불어민주당 PER ORG 30 32 100 105\n",
      "['특히', '김동연', '전', '경제', '##부', '##총리', '##를', '비롯', '##한', '김두관', '국회의원', ',', '@', '*', '[PER]', '*', '안', '##규', '##백', '@', '국회의원', ',', '김종', '##민', '국회의원', ',', '오', '##제', '##세', '국회의원', ',', '최', '##운', '##열', '국회의원', ',', '김정', '##우', '국회의원', ',', '권', '##칠', '##승', '국회의원', ',', '맹', '##성', '##규', '국회의원', '##등', '#', '^', '[ORG]', '^', '더불', '##어', '##민주당', '#', '국회의원', '8', '##명', '##이', '영상', '축하', '메세지', '##를', '보내', '눈길', '##을', '끌', '##었', '##다', '.']\n",
      "하비에르 파스토레는 아르헨티나 클럽 타예레스의 유소년팀에서 축구를 시작하였다. 하비에르 파스토레 아르헨티나 PER LOC 0 8 11 15\n",
      "['@', '*', '[PER]', '*', '하비', '##에르', '파스', '##토', '##레', '@', '##는', '#', '^', '[LOC]', '^', '아르헨티나', '#', '클럽', '타', '##예', '##레스', '##의', '유소년', '##팀', '##에서', '축구', '##를', '시작', '##하', '##였', '##다', '.']\n",
      "이른바 'Z세대'로 불리는 1990년대 중반 이후 태어난 세대에게 대표 아이콘으로 통하는 미국 싱어송라이터 빌리 아일리시(본명 빌리 오코널, 19)가 팝 역사를 새로 썼다. 빌리 아일리시 싱어송라이터 PER POH 60 66 53 58\n",
      "['이른바', \"'\", 'Z', '##세대', \"'\", '로', '불리', '##는', '1990', '##년', '##대', '중반', '이후', '태어난', '세대', '##에', '##게', '대표', '아이콘', '##으로', '통하', '##는', '미국', '#', '^', '[POH]', '^', '싱어', '##송', '##라이터', '#', '@', '*', '[PER]', '*', '빌리', '아', '##일리', '##시', '@', '(', '본명', '빌리', '오', '##코', '##널', ',', '19', ')', '가', '팝', '역사', '##를', '새로', '썼', '##다', '.']\n",
      "2009년 9월, 미국 프로 야구 필라델피아 필리스 소속의 야구 선수 박찬호는 《MBC 스페셜-박찬호는 당신을 잊지 않았다》 편에서 “최진실 씨의 아픔과 죽음의 고통을 이해합니다. 최진실 씨 사건에 눈물을 흘렸습니다. 저도 죽으려고 마음을 먹었던 적이 있었습니다. 잘하려고 애를 쓰는데 비난과 비판이 쏟아졌습니다. 머리가 빠지고 너무 힘들었습니다”라고 말하며 최진실의 죽음에 대해 안타까움을 표현했다. 필라델피아 필리스 박찬호 ORG PER 19 27 39 41\n",
      "['2009', '##년', '9', '##월', ',', '미국', '프로', '야구', '@', '*', '[ORG]', '*', '필라델피아', '필리', '##스', '@', '소속', '##의', '야구', '선수', '#', '^', '[PER]', '^', '박찬호', '#', '##는', '《', 'MBC', '스페셜', '-', '박찬호', '##는', '당신', '##을', '잊', '##지', '않', '##았', '##다', '》', '편', '##에서', '“', '최진', '##실', '씨', '##의', '아픔', '##과', '죽음', '##의', '고통', '##을', '이해', '##합니다', '.', '최진', '##실', '씨', '사건', '##에', '눈물', '##을', '흘렸', '##습', '##니다', '.', '저', '##도', '죽', '##으', '##려고', '마음', '##을', '먹', '##었', '##던', '적', '##이', '있', '##었', '##습', '##니다', '.', '잘', '##하', '##려고', '애', '##를', '쓰', '##는데', '비난', '##과', '비판', '##이', '쏟아졌', '##습', '##니다', '.', '머리', '##가', '빠지', '##고', '너무', '힘들', '##었', '##습', '##니다', '”', '라고', '말', '##하', '##며', '최진', '##실', '##의', '죽음', '##에', '대해', '안타까움', '##을', '표현', '##했', '##다', '.']\n",
      "뉴질랜드 1차산업부 생물보안 대변인 캐서린 더시 박사는 과일파리를 잡기 위해 오클랜드 전역에 1천300여개의 파리통을 놓았다며, 곧 통들을 마지막으로 점검할 것이라고 말했다. 뉴질랜드 오클랜드 ORG LOC 0 3 43 46\n",
      "['@', '*', '[ORG]', '*', '뉴질랜드', '@', '1', '##차', '##산업', '##부', '생물', '##보', '##안', '대변인', '캐서린', '더', '##시', '박사', '##는', '과일', '##파리', '##를', '잡', '##기', '위해', '#', '^', '[LOC]', '^', '오', '##클랜드', '#', '전역', '##에', '1', '##천', '##30', '##0', '##여', '##개', '##의', '파리', '##통', '##을', '놓', '##았', '##다', '##며', ',', '곧', '통', '##들', '##을', '마지막', '##으로', '점검', '##할', '것', '##이', '##라고', '말', '##했', '##다', '.']\n",
      "조토 디 본도네, 마사초, 피에로 델라 프란체스카, 도메니코 기를란다요, 페루지노, 미켈란젤로, 라파엘로, 보티첼리, 레오나르도 다빈치, 티치아노 등으로 대표되는 이탈리아 르네상스 회화는 당대와 그 이후의 시기 유럽 전체에 영향을 주었다. 미켈란젤로 르네상스 PER DAT 47 51 96 99\n",
      "['조', '##토', '디', '본', '##도', '##네', ',', '마사', '##초', ',', '피에', '##로', '델', '##라', '프란체', '##스카', ',', '도', '##메', '##니', '##코', '기를', '##란다', '##요', ',', '페루', '##지', '##노', ',', '@', '*', '[PER]', '*', '미켈란', '##젤로', '@', ',', '라파엘', '##로', ',', '보', '##티', '##첼', '##리', ',', '레오나르도', '다빈치', ',', '티', '##치아', '##노', '등', '##으로', '대표', '##되', '##는', '이탈리아', '#', '^', '[DAT]', '^', '르네상스', '#', '회화', '##는', '당대', '##와', '그', '이후', '##의', '시기', '유럽', '전체', '##에', '영향', '##을', '주', '##었', '##다', '.']\n",
      "성명에는 더불어민주당 이상민 대전광역시당 위원장, 이춘희 세종특별자치시당 위원장, 홍재형 충청북도당 위원장, 박수현 충청남도당 위원장, 이해찬, 박병석, 변재일, 오제세, 노영민, 양승조, 박범계, 박완주 의원이 서명했다. 노영민 더불어민주당 PER ORG 96 98 5 10\n",
      "['성명', '##에', '##는', '#', '^', '[ORG]', '^', '더불', '##어', '##민주당', '#', '이상민', '대전', '##광역시', '##당', '위원장', ',', '이춘', '##희', '세종', '##특별', '##자치', '##시', '##당', '위원장', ',', '홍', '##재', '##형', '충청', '##북도', '##당', '위원장', ',', '박수', '##현', '충청남도', '##당', '위원장', ',', '이해찬', ',', '박병', '##석', ',', '변', '##재', '##일', ',', '오', '##제', '##세', ',', '@', '*', '[PER]', '*', '노영', '##민', '@', ',', '양승', '##조', ',', '박범', '##계', ',', '박완', '##주', '의원', '##이', '서명', '##했', '##다', '.']\n",
      "이정미 대표도 이날 기자회견에 앞서 한 라디오 방송에 출연 \"정개특위를 한국당에 내주는 건 어떤 일이 있어도 안 된다\"며 \"지금 한국당을 어르고 구슬리는 게 민주당에 득이 된다고 생각할지 모르겠지만 정부 여당의 개혁 정책에 가장 힘을 실어줬던 정의당과 야3당의 개혁 공조가 어그러지는 상황이 될 것\"이라고 지적했다. 이정미 정의당 PER ORG 0 2 136 138\n",
      "['@', '*', '[PER]', '*', '이정미', '@', '대표', '##도', '이날', '기자', '##회', '##견', '##에', '앞서', '한', '라디오', '방송', '##에', '출연', '\"', '정개', '##특위', '##를', '한국', '##당', '##에', '내주', '##는', '건', '어떤', '일', '##이', '있', '##어도', '안', '된다', '\"', '며', '\"', '지금', '한국', '##당', '##을', '어르', '##고', '구슬', '##리', '##는', '게', '민주당', '##에', '득', '##이', '된다고', '생각', '##할', '##지', '모르', '##겠', '##지만', '정부', '여당', '##의', '개혁', '정책', '##에', '가장', '힘', '##을', '실', '##어', '##줬', '##던', '#', '^', '[ORG]', '^', '정의당', '#', '##과', '야', '##3', '##당', '##의', '개혁', '공조', '##가', '어', '##그러', '##지', '##는', '상황', '##이', '될', '것', '\"', '이라', '##고', '지적', '##했', '##다', '.']\n",
      "제2총군은 태평양 전쟁 말기에 일본 본토에 상륙하려는 연합군에게 대항하기 위해 설립된 일본 제국 육군의 총군이었다. 제2총군 일본 제국 육군 ORG ORG 0 3 48 55\n",
      "['@', '*', '[ORG]', '*', '제', '##2', '##총', '##군', '@', '##은', '태평양', '전쟁', '말기', '##에', '일본', '본토', '##에', '상륙', '##하', '##려', '##는', '연합군', '##에', '##게', '대항', '##하기', '위해', '설립', '##된', '#', '^', '[ORG]', '^', '일본', '제국', '육군', '#', '##의', '총', '##군', '##이', '##었', '##다', '.']\n",
      "문성민은 경기대학교에 입학하여 황동일, 신영석과 함께 경기대학교의 전성기를 이끌면서 하계대회, 전국체전, 최강전 등 3관왕을 이룬다. 문성민 경기대 PER ORG 0 2 5 7\n",
      "['@', '*', '[PER]', '*', '문성', '##민', '@', '##은', '#', '^', '[ORG]', '^', '경기', '##대', '#', '##학교', '##에', '입학', '##하여', '황', '##동', '##일', ',', '신영', '##석', '##과', '함께', '경기', '##대', '##학교', '##의', '전성기', '##를', '이끌', '##면서', '하계', '##대', '##회', ',', '전국', '##체', '##전', ',', '최강', '##전', '등', '3', '##관', '##왕', '##을', '이룬다', '.']\n",
      "이번 포럼은 제주개발공사와 유네스코 아시아-태평양 본부, 한국지질자원연구원이 공동 주최하고 유네스코 파리본부, 제주특자치도, 환경부, 한국수자원공사, 한국건설기술연구원 등이 후원한다. 한국수자원공사 환경부 ORG ORG 75 81 70 72\n",
      "['이번', '포럼', '##은', '제주', '##개발', '##공사', '##와', '유네스코', '아시아', '-', '태평양', '본부', ',', '한국', '##지', '##질', '##자원', '##연구원', '##이', '공동', '주최', '##하고', '유네스코', '파리', '##본부', ',', '제주', '##특', '##자치', '##도', ',', '#', '^', '[ORG]', '^', '환경부', '#', ',', '@', '*', '[ORG]', '*', '한국', '##수', '##자원', '##공사', '@', ',', '한국', '##건설', '##기술', '##연구원', '등', '##이', '후원', '##한다', '.']\n",
      "1971년 대선을 앞두고 김종필은 1971년 선거에서 박정희 당선을 위해 무려 600억원이나 썼다고 밝혔다. 김종필 박정희 PER PER 14 16 30 32\n",
      "['1971', '##년', '대선', '##을', '앞두', '##고', '@', '*', '[PER]', '*', '김종필', '@', '##은', '1971', '##년', '선거', '##에서', '#', '^', '[PER]', '^', '박정희', '#', '당선', '##을', '위해', '무려', '600', '##억', '##원', '##이나', '썼', '##다고', '밝혔', '##다', '.']\n",
      "2010년에는 아시아 가수 최초로 마이클 잭슨의 곡을 리메이크하였는데 당시 마이클 잭슨과 함께 작업했던 세계적인 뮤지션 스티브 바라캇(Steve Barakatt)과 마이클 잭슨 곡 \"You are not alone\"을 작업해 화제가 되었다. 스티브 바라캇 Steve Barakatt PER PER 67 73 75 88\n",
      "['2010', '##년', '##에', '##는', '아시아', '가수', '최초', '##로', '마이클', '잭슨', '##의', '곡', '##을', '리메이크', '##하', '##였', '##는데', '당시', '마이클', '잭슨', '##과', '함께', '작업', '##했', '##던', '세계', '##적인', '뮤지션', '@', '*', '[PER]', '*', '스티브', '[UNK]', '@', '(', 'St', '##ev', '##e', '@', '*', '[PER]', '*', '스티브', '[UNK]', '@', '(', '#', '^', '[PER]', '^', 'St', '##ev', '##e', 'Bar', '##ak', '##att', '#', ')', '과', '마이클', '잭슨', '곡', '\"', 'You', 'are', 'not', 'al', '##one', '\"', '을', '작업', '##해', '화제', '##가', '되', '##었', '##다', '.']\n",
      "박흥식은 첫 부인과의 사이에 장녀 박병숙을 두었고, 두 번째 부인은 경희대학교 교수를 지낸 피아니스트 한인하이며, 두 사람 사이에서 태어난 딸 박봉숙은 이화여자대학교 교수를 지냈다. 박흥식 한인하 PER PER 0 2 57 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-915f88fb0331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# tokenizing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../dataset/train/train_final.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-201-238bf14fdcde>\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file_in)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_start_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_end_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_start_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_end_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_ss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_os\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mrel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLABEL_TO_ID\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-201-238bf14fdcde>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, sentence, subject_word, object_word, subj_type, obj_type, ss, se, os, oe)\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mobj_token_collect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mowe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                         \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_token_collect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_token_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'^'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'^'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj_tokens\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj_token_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0mnew_os\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_token_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", \"-c\", type=str, default=\"1.2.0_config\")\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "conf = OmegaConf.load(f\"../code/config/{args.config}.yaml\")\n",
    "\n",
    "# load model and tokenizer\n",
    "MODEL_NAME = conf.model.model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# load dataset\n",
    "train_dataset = pd.read_csv(\"../dataset/train/train_final.csv\")\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = Processor(conf, tokenizer).read(\"../dataset/train/train_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='박흥식은 첫 부인과의 사이에 장녀 박병숙을 두었고, 두 번째 부인은 경희대학교 교수를 지낸 피아니스트 한인하이며, 두 사람 사이에서 태어난 딸 박봉숙은 이화여자대학교 교수를 지냈다.'\n",
    "subject_word = '박흥식' \n",
    "object_word = '한인하' \n",
    "subj_type, obj_type, ss, se, os, oe = 'PER', 'PER', 0, 2, 57, 59\n",
    "words=sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['박', '##흥', '##식', '##은'] ['박', '##흥', '##식']\n",
      "['한인', '##하이', '##며', ','] ['한인', '##하']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-227-d0eaa6319f49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubj_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-209-ec029cf97b2d>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sentence, subject_word, object_word, subj_type, obj_type, ss, se, os, oe)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mobj_token_collect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mowe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_token_collect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_token_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'^'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobj_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'^'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj_tokens\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobj_token_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mte\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mnew_os\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_token_collect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "tokenize(sentence, subject_word, object_word, subj_type, obj_type, ss, se, os, oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def word_idx_extract( words, ns, ne):\n",
    "        \n",
    "        word_indices = []\n",
    "        start_index = 0\n",
    "\n",
    "        for word in words:\n",
    "            end_index = start_index + len(word) - 1\n",
    "            word_indices.append((start_index, end_index))\n",
    "            start_index = end_index + 2\n",
    "        word_idx=[]\n",
    "        for i, (start, end) in enumerate(word_indices):\n",
    "            if ns in range(start, end + 1) or ne in range(start, end + 1):\n",
    "                word_idx.append(i)\n",
    "        return word_idx[0] , word_idx[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in range(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def word_idx_extract( words, ns, ne):\n",
    "        \n",
    "        word_indices = []\n",
    "        start_index = 0\n",
    "\n",
    "        for word in words:\n",
    "            end_index = start_index + len(word) - 1\n",
    "            word_indices.append((start_index, end_index))\n",
    "            start_index = end_index + 2\n",
    "\n",
    "        word_idx=[]\n",
    "        for i, (start, end) in enumerate(word_indices):\n",
    "            if ns in range(start, end + 1) or ne in range(start, end + 1):\n",
    "                word_idx.append(i)\n",
    "                \n",
    "        return word_idx[0] , word_idx[-1]\n",
    "\n",
    "\n",
    "    def token_location(list1, list2):\n",
    "        print(list1, list2)\n",
    "        for i in range(len(list1) - len(list2) + 1):\n",
    "            if list1[i:i + len(list2)] == list2:\n",
    "                index = i\n",
    "                return i, i+len(list2)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def tokenize(sentence, subject_word, object_word, subj_type, obj_type, ss, se, os, oe):\n",
    "        \n",
    "        words = sentence.split()\n",
    "\n",
    "        sws, swe = word_idx_extract(words, ss,se)\n",
    "        ows, owe = word_idx_extract(words, os,oe)\n",
    "\n",
    "        subj_tokens= tokenizer.tokenize(subject_word)\n",
    "        obj_tokens= tokenizer.tokenize(object_word)\n",
    "\n",
    "        sents =[]\n",
    "        subj_type , obj_type = f\"[{subj_type}]\", f\"[{obj_type}]\"\n",
    "        subj_token_collect = []\n",
    "        obj_token_collect = []\n",
    "\n",
    "        for idx, word in enumerate(words):\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            if idx not in range(sws,swe+1) and idx not in range(ows,owe+1):\n",
    "                sents.extend(tokens)\n",
    "\n",
    "            else:\n",
    "                if sws <= idx and idx <= swe:\n",
    "                    subj_token_collect.extend(tokens)\n",
    "                    if idx == swe:\n",
    "                        ts, te = token_location(subj_token_collect, subj_tokens)\n",
    "                        tokens = subj_token_collect[:ts] + ['@'] + ['*'] + [subj_type] + ['*'] + subj_tokens + ['@'] + subj_token_collect[te+1:]\n",
    "                        new_ss = len(sents) + len(subj_token_collect[:ts])\n",
    "                        sents.extend(tokens)\n",
    "\n",
    "                if ows <= idx and idx <= owe:\n",
    "                    obj_token_collect.extend(tokens)\n",
    "                    if idx == owe:\n",
    "                        ts, te = token_location(obj_token_collect, obj_tokens)\n",
    "                        tokens = obj_token_collect[:ts] + [\"#\"] + ['^'] + [obj_type] + ['^'] + obj_tokens + [\"#\"] + obj_token_collect[te+1:]\n",
    "                        new_os = len(sents) + len(obj_token_collect[:ts])\n",
    "                        sents.extend(tokens)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(sents)\n",
    "        input_ids = tokenizer.build_inputs_with_special_tokens(input_ids)\n",
    "        print(sents)\n",
    "        \n",
    "        return input_ids, new_ss + 1, new_os + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", \"-c\", type=str, default=\"1.2.0_config\")\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "conf = OmegaConf.load(f\"../code/config/{args.config}.yaml\")\n",
    "\n",
    "# load model and tokenizer\n",
    "MODEL_NAME = conf.model.model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# load dataset\n",
    "train_dataset = pd.read_csv(\"../dataset/train/train_final.csv\")\n",
    "train_label = label_to_num(train_dataset['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=next(train_dataset.iterrows())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = example['sentence']\n",
    "subject_word,object_word = example['subject_word'],  example['object_word']\n",
    "ss, se = example['subject_start_idx'], example['subject_end_idx']\n",
    "os, oe = example['object_start_idx'], example['object_end_idx']\n",
    "subject_type, oject_type = example['subject_type'], example['object_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['〈', 'So', '##me', '##th', '##ing', '〉', '는', '#', '^', '[PER]', '^', '조지', '해리', '##슨', '#', '##이', '쓰', '##고', '@', '*', '[ORG]', '*', '비틀즈', '@', '##가', '1969', '##년', '앨범', '《', 'Ab', '##be', '##y', 'Ro', '##ad', '》', '에', '담', '##은', '노래', '##다', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2,\n",
       "  168,\n",
       "  30985,\n",
       "  14451,\n",
       "  7088,\n",
       "  4586,\n",
       "  169,\n",
       "  793,\n",
       "  7,\n",
       "  65,\n",
       "  1,\n",
       "  65,\n",
       "  8373,\n",
       "  14113,\n",
       "  2234,\n",
       "  7,\n",
       "  2052,\n",
       "  1363,\n",
       "  2088,\n",
       "  36,\n",
       "  14,\n",
       "  1,\n",
       "  14,\n",
       "  29830,\n",
       "  36,\n",
       "  2116,\n",
       "  14879,\n",
       "  2440,\n",
       "  6711,\n",
       "  170,\n",
       "  21406,\n",
       "  26713,\n",
       "  2076,\n",
       "  25145,\n",
       "  5749,\n",
       "  171,\n",
       "  1421,\n",
       "  818,\n",
       "  2073,\n",
       "  4388,\n",
       "  2062,\n",
       "  18,\n",
       "  3],\n",
       " 19,\n",
       " 8)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(sentence, subject_word, object_word, subj_type, obj_type, ss, se, os, oe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(self, args, tokenizer):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.new_tokens = ['[PER]', '[ORG]', '[DAT]', '[LOC]', '[POH]', '[NOH]']\n",
    "        self.tokenizer.add_tokens(self.new_tokens)\n",
    "        self.LABEL_TO_ID = {'no_relation': 0, 'org:top_members/employees': 1, 'org:members': 2, 'org:product': 3, 'per:title': 4, 'org:alternate_names': 5, 'per:employee_of': 6, \\\n",
    "                'org:place_of_headquarters': 7, 'per:product': 8, 'org:number_of_employees/members': 9, 'per:children': 10, 'per:place_of_residence': 11, 'per:alternate_names': 12, \\\n",
    "                'per:other_family': 13, 'per:colleagues': 14, 'per:origin': 15, 'per:siblings': 16, 'per:spouse': 17, 'org:founded': 18, 'org:political/religious_affiliation': 19, \\\n",
    "                'org:member_of': 20, 'per:parents': 21, 'org:dissolved': 22, 'per:schools_attended': 23, 'per:date_of_death': 24, 'per:date_of_birth': 25, 'per:place_of_birth': 26, \\\n",
    "                'per:place_of_death': 27, 'org:founded_by': 28, 'per:religion': 29}\n",
    "        \n",
    "    def token_location(self, list1, list2):\n",
    "        for idx in range(len(list1) - len(list2) + 1):\n",
    "            if list1[idx:idx + len(list2)] == list2:\n",
    "                index = idx\n",
    "                return idx\n",
    "            \n",
    "    def tokenize(self, sentence, subj_type, obj_type, ss, se, os, oe):\n",
    "        \n",
    "        subj_type , obj_type = f\"[{subj_type}]\", f\"[{obj_type}]\"\n",
    "        new_sentence=''\n",
    "        if ss < os:\n",
    "            new_sentence += sentence[ :ss]\n",
    "            new_sentence += f\"@*{subj_type}*{sentence[ss:se+1]}@\"\n",
    "            new_sentence += sentence[se+1:os]a\n",
    "            new_sentence += f\"#^{obj_type}^{sentence[os:oe+1]}#\"\n",
    "            new_sentence += sentence[oe+1:]\n",
    "        else:\n",
    "            new_sentence += sentence[ :os]\n",
    "            new_sentence += f\"#^{obj_type}^{sentence[os:oe+1]}#\"\n",
    "            new_sentence += sentence[oe+1:ss]\n",
    "            new_sentence += f\"@*{subj_type}*{sentence[ss:se+1]}@\"\n",
    "            new_sentence += sentence[se+1:]\n",
    "\n",
    "        sents=self.tokenizer.tokenize(new_sentence)\n",
    "        new_ss= self.token_location(sents,['@',\"*\"])\n",
    "        new_os= self.token_location(sents,['#',\"^\"])\n",
    "\n",
    "        sents = sents[:self.args.model.max_seq_length - 2]\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(sents)\n",
    "        input_ids = self.tokenizer.build_inputs_with_special_tokens(input_ids)\n",
    "        return input_ids, new_ss + 1, new_os + 1\n",
    "\n",
    "    def read(self, file_in):\n",
    "        features = []\n",
    "        with open(file_in, \"r\") as fh:\n",
    "            data = pd.read_csv(fh)\n",
    "\n",
    "        for _, d in tqdm(data.iterrows()):\n",
    "            ss, se = int(d['subject_start_idx']), int(d['subject_end_idx'])\n",
    "            os, oe = int(d['object_start_idx']), int(d['object_end_idx'])\n",
    "            input_ids, new_ss, new_os = self.tokenize(d['sentence'], d['subject_type'], d['object_type'], ss, se, os, oe)\n",
    "            rel = self.LABEL_TO_ID[d['label']]\n",
    "\n",
    "            feature = {\n",
    "                'input_ids': input_ids,\n",
    "                'labels': rel,\n",
    "                'ss': new_ss,\n",
    "                'os': new_os,\n",
    "            }\n",
    "            features.append(feature)\n",
    "\n",
    "        return features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32470it [00:27, 1178.71it/s]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--config\", \"-c\", type=str, default=\"1.2.0_config\")\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "conf = OmegaConf.load(f\"../code/config/{args.config}.yaml\")\n",
    "\n",
    "# load model and tokenizer\n",
    "MODEL_NAME = conf.model.model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# load dataset\n",
    "train_dataset = pd.read_csv(\"../dataset/train/train_final.csv\")\n",
    "train_label = label_to_num(train_dataset['label'].values)\n",
    "\n",
    "# tokenizing dataset\n",
    "tokenized_train = Processor(conf, tokenizer).read(\"../dataset/train/train_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 36, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  168,\n",
       "  30985,\n",
       "  14451,\n",
       "  7088,\n",
       "  4586,\n",
       "  169,\n",
       "  793,\n",
       "  7,\n",
       "  65,\n",
       "  32000,\n",
       "  65,\n",
       "  8373,\n",
       "  14113,\n",
       "  2234,\n",
       "  7,\n",
       "  1504,\n",
       "  1363,\n",
       "  2088,\n",
       "  36,\n",
       "  14,\n",
       "  32001,\n",
       "  14,\n",
       "  29830,\n",
       "  36,\n",
       "  543,\n",
       "  14879,\n",
       "  2440,\n",
       "  6711,\n",
       "  170,\n",
       "  21406,\n",
       "  26713,\n",
       "  2076,\n",
       "  25145,\n",
       "  5749,\n",
       "  171,\n",
       "  1421,\n",
       "  818,\n",
       "  2073,\n",
       "  4388,\n",
       "  2062,\n",
       "  18,\n",
       "  3],\n",
       " 'labels': 0,\n",
       " 'ss': 19,\n",
       " 'os': 8}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "def add_typed_entity_marker_original(file_path):\n",
    "    \"\"\"기존각 엔티티 토큰을 스페셜 토큰에 추가하는 방식\n",
    "\n",
    "    Args:\n",
    "        file_path (str): csv파일경로\n",
    "\n",
    "    Returns:\n",
    "        list: 스페셜 토큰에 추가할 엔티티 토큰의 list\n",
    "    \"\"\"\n",
    "    entity_tokens = set()\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        subject_entity = ast.literal_eval(row['subject_entity'])\n",
    "        object_entity = ast.literal_eval(row['object_entity'])\n",
    "\n",
    "        new_sentence = ''\n",
    "\n",
    "        curr_entity_tokens = [f'<S:{subject_entity[\"type\"]}>',\n",
    "                              f'</S:{subject_entity[\"type\"]}>',\n",
    "                              f'<O:{object_entity[\"type\"]}>',\n",
    "                              f'</O:{object_entity[\"type\"]}>']\n",
    "        entity_tokens.update(curr_entity_tokens)\n",
    "\n",
    "        if subject_entity['start_idx'] < object_entity['start_idx']:\n",
    "            new_sentence += sentence[ :subject_entity['start_idx'] ]\n",
    "            new_sentence += curr_entity_tokens[0]\n",
    "            new_sentence += sentence[ subject_entity['start_idx']:subject_entity['end_idx']+1 ]\n",
    "            new_sentence += curr_entity_tokens[1]\n",
    "            new_sentence += sentence[ subject_entity['end_idx']+1:object_entity['start_idx'] ]\n",
    "            new_sentence += curr_entity_tokens[2]\n",
    "            new_sentence += sentence[ object_entity['start_idx']:object_entity['end_idx']+1 ]\n",
    "            new_sentence += curr_entity_tokens[3]\n",
    "            new_sentence += sentence[ object_entity['end_idx']+1: ]\n",
    "        else:\n",
    "            new_sentence += sentence[ :object_entity['start_idx'] ]\n",
    "            new_sentence += curr_entity_tokens[2]\n",
    "            new_sentence += sentence[ object_entity['start_idx']:object_entity['end_idx']+1 ]\n",
    "            new_sentence += curr_entity_tokens[3]\n",
    "            new_sentence += sentence[ object_entity['end_idx']+1:subject_entity['start_idx'] ]\n",
    "            new_sentence += curr_entity_tokens[0]\n",
    "            new_sentence += sentence[ subject_entity['start_idx']:subject_entity['end_idx']+1 ]\n",
    "            new_sentence += curr_entity_tokens[1]\n",
    "            new_sentence += sentence[ subject_entity['end_idx']+1: ]\n",
    "\n",
    "        result.append(new_sentence)\n",
    "    \n",
    "    df['sentence'] = result\n",
    "    df.to_csv('typed_entity_marker_original_train.csv')\n",
    "\n",
    "    return list(entity_tokens) \n",
    "\n",
    "\n",
    "def add_typed_entity_marker_punct(file_path):\n",
    "    \"\"\"각 엔티티 토큰은 @, *, &, ^로 고정. 다만 논문과 다르게 우리가 사용할 한글 토크나이저에\n",
    "       1. 엔티티 타입이 없을 수 있어 [UNK]으로 되는것을 방지하고자 엔티티 타입 자체를 스페셜 토큰으로 추가\n",
    "       2. wordpiece 토크나이저를 사용할 수 있으므로 object entity의 토큰을 논문의 '#'대신 '&'로 대체사용 \n",
    "   \n",
    "    Args:\n",
    "        file_path (str): csv파일경로\n",
    "\n",
    "    Returns:\n",
    "        list: 스페셜 토큰에 추가할 엔티티 타입의 list\n",
    "    \"\"\"\n",
    "    entity_tokens = set()\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        subject_entity = ast.literal_eval(row['subject_entity'])\n",
    "        object_entity = ast.literal_eval(row['object_entity'])\n",
    "\n",
    "        new_sentence = ''\n",
    "\n",
    "        curr_entity_tokens = [subject_entity[\"type\"],\n",
    "                              object_entity[\"type\"]]\n",
    "        entity_tokens.update(curr_entity_tokens)\n",
    "\n",
    "        if subject_entity['start_idx'] < object_entity['start_idx']:\n",
    "            new_sentence += sentence[ :subject_entity['start_idx'] ]\n",
    "            new_sentence += f\"@*{curr_entity_tokens[0]}*{sentence[subject_entity['start_idx']:subject_entity['end_idx']+1]}@\"\n",
    "            new_sentence += sentence[subject_entity['end_idx']+1:object_entity['start_idx']]\n",
    "            new_sentence += f\"&^{curr_entity_tokens[1]}^{sentence[object_entity['start_idx']:object_entity['end_idx']+1]}&\"\n",
    "            new_sentence += sentence[object_entity['end_idx']+1:]\n",
    "        else:\n",
    "            new_sentence += sentence[ :object_entity['start_idx'] ]\n",
    "            new_sentence += f\"&^{curr_entity_tokens[1]}^{sentence[object_entity['start_idx']:object_entity['end_idx']+1]}&\"\n",
    "            new_sentence += sentence[object_entity['end_idx']+1:subject_entity['start_idx']]\n",
    "            new_sentence += f\"@*{curr_entity_tokens[0]}*{sentence[subject_entity['start_idx']:subject_entity['end_idx']+1]}@\"\n",
    "            new_sentence += sentence[subject_entity['end_idx']+1:]\n",
    "        result.append(new_sentence)\n",
    "    \n",
    "    df['sentence'] = result\n",
    "    df.to_csv('typed_entity_marker_punct_train.csv')\n",
    "\n",
    "    return list(entity_tokens) \n",
    "\n",
    "# add_typed_entity_marker_original(\"../dataset/train/train.csv\")\n",
    "# add_typed_entity_marker_punct(\"../dataset/train/train.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, Trainer, TrainingArguments\n",
    "from omegaconf import OmegaConf\n",
    "from dataset_utils import label_to_num\n",
    "from datasets import RE_Dataset\n",
    "from metrics import compute_metrics\n",
    "from custom_model import Custom_Model\n",
    "from custom_prepros import Processor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "\n",
    "def set_seed(seed:int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\"--config\", \"-c\", type=str, default=\"1.2.0_config\")\n",
    "\n",
    "  args, _ = parser.parse_known_args()\n",
    "  conf = OmegaConf.load(f\"../code/config/{args.config}.yaml\")\n",
    "\n",
    "  set_seed(42)\n",
    "\n",
    "  wandb.login()\n",
    "  wandb.init(project=conf.wandb.project_name)\n",
    "\n",
    "  # load model and tokenizer\n",
    "  MODEL_NAME = conf.model.model_name\n",
    "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "  # load dataset\n",
    "  train_dataset = pd.read_csv(\"../dataset/train/train_final.csv\")\n",
    "  train_label = label_to_num(train_dataset['label'].values)\n",
    "\n",
    "  # tokenizing dataset\n",
    "  tokenized_train = Processor(conf, tokenizer).read(\"../dataset/train/train_final.csv\")\n",
    "\n",
    "  # make dataset for pytorch.\n",
    "  RE_train_dataset = RE_Dataset(tokenized_train, train_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
